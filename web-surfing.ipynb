!sudo apt-get update -y
!sudo apt-get install -y curl pciutils lsb-release
!curl -fsSL https://ollama.com/install.sh | sh

# -----------------------------------------------------------
# üåç Web-Surfing QA Bot ‚Äì Colab, SerpAPI (direct HTTP)
# -----------------------------------------------------------
# * Uses SerpAPI free tier (100 searches/day) ‚Äì no client lib needed
# * Scrapes result pages ‚Üí embeds with Sentence-Transformers
# * Answers with FLAN-T5 (open-source) + cites sources
# -----------------------------------------------------------
# üëâ  Set your key in Colab before running any cells:
#     import os, getpass, json, requests
#     os.environ["SERPAPI_API_KEY"] = getpass.getpass("üîë Enter SERPAPI_API_KEY: ")
# -----------------------------------------------------------

# CELL 1 ‚Äì Install dependencies
!pip -q install beautifulsoup4 requests langchain-core langchain-community langchain-chroma \
               sentence-transformers chromadb transformers accelerate --upgrade

# CELL 2 ‚Äì Imports & LLM/Embedding setup
import os, requests, html
from typing import List
from bs4 import BeautifulSoup

# LangChain + Transformers
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.llms import HuggingFacePipeline
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

# Embeddings
embedder = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

# Lightweight LLM
MODEL_NAME = "google/flan-t5-base"
_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
_model     = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
llm_pipe   = pipeline("text2text-generation", model=_model, tokenizer=_tokenizer, max_new_tokens=256)
llm        = HuggingFacePipeline(pipeline=llm_pipe)

splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)

# CELL 3 ‚Äì SerpAPI search via raw HTTP
SERP_KEY = os.getenv("SERPAPI_API_KEY")
if not SERP_KEY:
    import getpass, os
    SERP_KEY = getpass.getpass("üîë Enter SERPAPI_API_KEY: ")
    os.environ["SERPAPI_API_KEY"] = SERP_KEY

HEADERS = {"User-Agent": "Mozilla/5.0"}

def serp_search(query: str, max_results: int = 10) -> List[str]:
    params = {
        "engine": "google",
        "q": query,
        "api_key": SERP_KEY,
        "num": max_results,
        "hl": "en",
        "safe": "active",
    }
    try:
        r = requests.get("https://serpapi.com/search.json", params=params, timeout=15)
        r.raise_for_status()
        data = r.json()
        return [item["link"] for item in data.get("organic_results", []) if item.get("link")]
    except Exception as e:
        print("[SerpAPI error]", e)
        return []

def fetch_page_text(url: str) -> str:
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        if not resp.ok or "text" not in resp.headers.get("content-type", ""):
            return ""
        soup = BeautifulSoup(resp.text, "html.parser")
        for t in soup(["script", "style", "noscript"]):
            t.decompose()
        return " ".join(soup.get_text(" ", strip=True).split())[:30000]
    except Exception as e:
        print("[skip]", url, e)
        return ""

def build_docs(query: str, max_sites: int = 10) -> List[Document]:
    docs = []
    for url in serp_search(query, max_sites):
        txt = fetch_page_text(url)
        if len(txt) >= 150:
            docs.append(Document(page_content=txt, metadata={"source": url}))
    return docs

# CELL 4 ‚Äì RAG chain builder
PROMPT = PromptTemplate(
    input_variables=["context", "question"],
    template=(
        "Answer ONLY using the context below.\n"
        "If unsure, say: 'I don't know based on the provided webpages.'\n\n"
        "Context:\n{context}\n\n"
        "Question: {question}\nAnswer:"
    ),
)

def build_chain(docs: List[Document]):
    vector = Chroma.from_documents(splitter.split_documents(docs), embedding=embedder)
    return RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vector.as_retriever(search_kwargs={"k": 4}),
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )

# CELL 5 ‚Äì Gradio UI
import gradio as gr

def answer_fn(q: str):
    q = (q or "").strip()
    if not q:
        return "‚ö†Ô∏è Please ask a question."
    docs = build_docs(q, max_sites=10)
    if not docs:
        return "‚ö†Ô∏è No suitable pages found."
    chain = build_chain(docs)
    res = chain.invoke({"query": q})
    ans = res["result"]
    sources = {d.metadata.get("source", "") for d in res.get("source_documents", [])}
    foot = "\n\n**Sources**:\n" + "\n".join(sorted(sources)) if sources else ""
    return ans + foot

with gr.Blocks(title="SerpAPI Web QA Bot üåç") as demo:
    gr.Markdown("## üåç Ask anything ‚Äì Google (SerpAPI) + FLAN-T5 answers")
    inp = gr.Textbox(label="Your question", placeholder="e.g. What is Docker?", lines=1)
    out = gr.Markdown()
    btn = gr.Button("Search & Answer")
    btn.click(answer_fn, inputs=inp, outputs=out)
    gr.Markdown("*Powered by SerpAPI, Sentence-Transformers & FLAN-T5.*")

demo.launch(debug=True)
